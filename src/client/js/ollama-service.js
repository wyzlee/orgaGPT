// ollama-service.js - Service direct pour l'int√©gration d'Ollama
// Ce fichier remplace directement la m√©thode sendChatMessage pour utiliser Ollama

(function() {
    // Attendre que le DOM soit charg√©
    document.addEventListener('DOMContentLoaded', () => {
        // Attendre que orgaGPT soit initialis√© (500ms devrait suffire)
        setTimeout(() => {
            if (!window.orgaGPT) {
                console.error('‚ùå Impossible de trouver orgaGPT');
                return;
            }

            console.log('üöÄ Initialisation du service Ollama...');
            
            // Ajouter un indicateur visuel dans l'interface
            const aiTitle = document.querySelector('.sidebar__section h3');
            if (aiTitle && aiTitle.textContent.includes('Copilote IA')) {
                aiTitle.innerHTML = 'ü§ñ Copilote IA <span style="color: green; font-size: 0.8em; background: rgba(0,255,0,0.1); padding: 2px 6px; border-radius: 10px;">‚úì OLLAMA ACTIF</span>';
            }
            
            // Ajouter un indicateur dans le chat
            const chatMessages = document.getElementById('chatMessages');
            if (chatMessages) {
                const statusMessage = document.createElement('div');
                statusMessage.className = 'ai-message system-message';
                statusMessage.innerHTML = `<strong>Syst√®me:</strong> IA Ollama (gemma3:1b) activ√©e. Posez vos questions!`;
                chatMessages.appendChild(statusMessage);
                
                // Scroll to bottom
                setTimeout(() => {
                    chatMessages.scrollTop = chatMessages.scrollHeight;
                }, 100);
            }
            
            // Remplacer compl√®tement la m√©thode sendChatMessage
            window.orgaGPT.sendChatMessage = async function() {
                const input = document.getElementById('chatInput');
                const message = input.value.trim();
                
                if (!message || !input) return;

                // Ajouter le message utilisateur
                this.addUserMessage(message);
                input.value = '';
                
                // Cr√©er l'indicateur de chargement
                const loadingDiv = document.createElement('div');
                loadingDiv.className = 'ai-message typing-indicator';
                loadingDiv.innerHTML = '<span>.</span><span>.</span><span>.</span>';
                chatMessages.appendChild(loadingDiv);
                
                // Scroll to bottom
                chatMessages.scrollTop = chatMessages.scrollHeight;
                
                try {
                    console.log('üîÑ Envoi de la requ√™te √† Ollama...');
                    
                    // Construire un contexte riche pour l'IA
                    const context = {
                        tasks: this.tasks,
                        completedTasks: this.tasks.filter(t => t.completed).length,
                        totalTasks: this.tasks.length,
                        currentView: this.currentView,
                        energyLevel: document.getElementById('energyLevel')?.value || 'medium',
                        pomodoroCount: this.pomodoroCount,
                        focusTime: Math.round(this.totalFocusTime / 60)
                    };
                    
                    // Cr√©er un prompt riche
                    const prompt = `Tu es OrgaGPT, un assistant de productivit√© expert bas√© sur les recherches des grands cabinets de conseil (McKinsey, Deloitte, BCG, KPMG).

Contexte actuel de l'utilisateur:
- ${context.totalTasks} t√¢ches dont ${context.completedTasks} compl√©t√©es
- Vue actuelle: ${context.currentView}
- Niveau d'√©nergie: ${context.energyLevel}
- Sessions Pomodoro: ${context.pomodoroCount}
- Temps de focus: ${context.focusTime}h

Question: ${message}

R√©ponds de mani√®re concise et actionnable. Utilise des √©mojis pertinents et cite les sources quand tu mentionnes des statistiques.`;

                    // Appel √† Ollama
                    const response = await fetch('http://localhost:11434/api/generate', {
                        method: 'POST',
                        headers: { 'Content-Type': 'application/json' },
                        body: JSON.stringify({
                            model: 'gemma3:1b',
                            prompt: prompt,
                            stream: false
                        })
                    });
                    
                    if (!response.ok) {
                        throw new Error(`Erreur HTTP: ${response.status}`);
                    }
                    
                    const data = await response.json();
                    
                    // Supprimer l'indicateur de chargement
                    loadingDiv.remove();
                    
                    // Ajouter la r√©ponse de l'IA
                    this.addAIMessage(data.response);
                    console.log('‚úÖ R√©ponse IA re√ßue avec succ√®s!');
                    
                } catch (error) {
                    console.error('‚ùå Erreur avec Ollama:', error);
                    
                    // Supprimer l'indicateur de chargement
                    loadingDiv.remove();
                    
                    // Message d'erreur
                    const errorDiv = document.createElement('div');
                    errorDiv.className = 'ai-message error-message';
                    errorDiv.innerHTML = `<strong>Erreur de connexion √† Ollama:</strong> ${error.message}<br><br>Assurez-vous qu'Ollama est bien lanc√© avec:<br><code>ollama serve</code><br><br>Je vais utiliser une r√©ponse locale en attendant.`;
                    chatMessages.appendChild(errorDiv);
                    
                    // Fallback vers la m√©thode originale apr√®s un d√©lai
                    setTimeout(() => {
                        const fallbackResponse = this.generateAIResponse(message);
                        this.addAIMessage(fallbackResponse + '\n\n[Note: R√©ponse g√©n√©r√©e localement]');
                    }, 1500);
                }
            };
            
            // Styles pour les indicateurs
            const styles = `
            <style>
                .typing-indicator {
                    display: flex;
                    align-items: center;
                    column-gap: 4px;
                }
                
                .typing-indicator span {
                    animation: blink 1s infinite;
                    font-size: 20px;
                }
                
                .typing-indicator span:nth-child(2) {
                    animation-delay: 0.2s;
                }
                
                .typing-indicator span:nth-child(3) {
                    animation-delay: 0.4s;
                }
                
                @keyframes blink {
                    0% { opacity: 0.2; }
                    20% { opacity: 1; }
                    100% { opacity: 0.2; }
                }
                
                .system-message {
                    background-color: rgba(0, 128, 255, 0.1);
                    border-left: 3px solid #0080ff;
                }
                
                .error-message {
                    background-color: rgba(255, 0, 0, 0.1);
                    border-left: 3px solid #ff0000;
                }
                
                code {
                    background: rgba(0,0,0,0.1);
                    padding: 2px 4px;
                    border-radius: 4px;
                    font-family: monospace;
                }
            </style>
            `;
            
            document.head.insertAdjacentHTML('beforeend', styles);
            
            console.log('‚úÖ Service Ollama initialis√© avec succ√®s!');
        }, 500);
    });
})();
